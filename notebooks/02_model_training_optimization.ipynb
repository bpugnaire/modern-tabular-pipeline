{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & Optimization - Churn Prediction\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Baseline Training**: First training run using our pipeline\n",
    "2. **Model Evaluation**: Performance analysis and diagnostics\n",
    "3. **Feature Importance**: Understanding key drivers\n",
    "4. **Optimization Opportunities**: Hyperparameter tuning strategies\n",
    "5. **Model Interpretation**: SHAP values and explainability\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from catboost import CatBoostClassifier\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / '.env')\n",
    "\n",
    "# Project imports\n",
    "from src.data.loaders import load_features_from_gcs\n",
    "from src.data.schemas import ChurnFeatureSchema\n",
    "from src.models.catboost import CatBoostModel\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validated features\n",
    "df_features = load_features_from_gcs(\n",
    "    \"gs://modern-tabular-dev/data/features/churn_features.parquet\",\n",
    "    os.getenv('GCS_KEY_ID'),\n",
    "    os.getenv('GCS_SECRET'),\n",
    ")\n",
    "\n",
    "df = df_features.to_pandas()\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ChurnFeatureSchema.get_feature_columns()\n",
    "categorical_cols = ChurnFeatureSchema.get_categorical_columns()\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df['has_churned']\n",
    "\n",
    "# Ensure proper types\n",
    "for col in categorical_cols:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].astype(str)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]:,}\")\n",
    "print(f\"Churn rate: {y.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train):,} samples ({y_train.mean():.2%} churn)\")\n",
    "print(f\"Test set: {len(X_test):,} samples ({y_test.mean():.2%} churn)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model Training\n",
    "\n",
    "Train a CatBoost model with default parameters as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "baseline_model = CatBoostModel(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Training baseline model...\")\n",
    "baseline_model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "print(\"✓ Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "y_pred_proba = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1 Score': f1_score(y_test, y_pred),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "metrics_df = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "metrics_df['Value'] = metrics_df['Value'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig = px.imshow(cm, \n",
    "                text_auto=True,\n",
    "                labels=dict(x=\"Predicted\", y=\"Actual\"),\n",
    "                x=['No Churn', 'Churn'],\n",
    "                y=['No Churn', 'Churn'],\n",
    "                title='Confusion Matrix',\n",
    "                color_continuous_scale='Blues')\n",
    "fig.update_xaxes(side=\"bottom\")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', \n",
    "                         name=f'ROC Curve (AUC = {roc_auc:.4f})',\n",
    "                         line=dict(color='darkorange', width=2)))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines',\n",
    "                         name='Random Classifier',\n",
    "                         line=dict(color='navy', width=2, dash='dash')))\n",
    "\n",
    "fig.update_layout(title='ROC Curve',\n",
    "                  xaxis_title='False Positive Rate',\n",
    "                  yaxis_title='True Positive Rate',\n",
    "                  width=700, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=recall, y=precision, mode='lines',\n",
    "                         name='Precision-Recall Curve',\n",
    "                         line=dict(color='darkgreen', width=2)))\n",
    "\n",
    "fig.update_layout(title='Precision-Recall Curve',\n",
    "                  xaxis_title='Recall',\n",
    "                  yaxis_title='Precision',\n",
    "                  width=700, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = baseline_model.get_feature_importance()\n",
    "importance_df = pd.DataFrame(list(importance.items()), \n",
    "                             columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Top 20 features\n",
    "top_features = importance_df.head(20)\n",
    "\n",
    "fig = px.bar(top_features.sort_values('Importance'), \n",
    "             x='Importance', y='Feature',\n",
    "             orientation='h',\n",
    "             title='Top 20 Feature Importances',\n",
    "             labels={'Importance': 'Importance Score'})\n",
    "fig.update_layout(height=600)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nTop 10 Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation with SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides model-agnostic feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "print(\"Computing SHAP values (this may take a minute)...\")\n",
    "explainer = shap.TreeExplainer(baseline_model.model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "print(\"✓ SHAP values computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title(\"SHAP Feature Importance\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot (detailed)\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test, show=False, max_display=20)\n",
    "plt.title(\"SHAP Summary Plot - Feature Impact on Predictions\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot for top feature\n",
    "top_feature = importance_df.iloc[0]['Feature']\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.dependence_plot(top_feature, shap_values, X_test, show=False)\n",
    "plt.title(f\"SHAP Dependence Plot - {top_feature}\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false positives and false negatives\n",
    "results_df = X_test.copy()\n",
    "results_df['true_label'] = y_test.values\n",
    "results_df['predicted_label'] = y_pred\n",
    "results_df['predicted_proba'] = y_pred_proba\n",
    "results_df['correct'] = results_df['true_label'] == results_df['predicted_label']\n",
    "\n",
    "# False positives (predicted churn, but didn't churn)\n",
    "false_positives = results_df[(results_df['true_label'] == False) & \n",
    "                             (results_df['predicted_label'] == True)]\n",
    "\n",
    "# False negatives (didn't predict churn, but churned)\n",
    "false_negatives = results_df[(results_df['true_label'] == True) & \n",
    "                             (results_df['predicted_label'] == False)]\n",
    "\n",
    "print(f\"False Positives: {len(false_positives)} ({len(false_positives)/len(results_df):.2%})\")\n",
    "print(f\"False Negatives: {len(false_negatives)} ({len(false_negatives)/len(results_df):.2%})\")\n",
    "\n",
    "# Analyze false negatives (more costly - missed churners)\n",
    "if len(false_negatives) > 0:\n",
    "    print(\"\\nFalse Negative Characteristics (Missed Churners):\")\n",
    "    print(false_negatives[['tenure_months', 'monthly_charges', 'contract_type', \n",
    "                           'churn_risk_score', 'predicted_proba']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction confidence distribution\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(x=results_df[results_df['true_label'] == False]['predicted_proba'],\n",
    "                           name='Actual No Churn',\n",
    "                           opacity=0.7,\n",
    "                           nbinsx=30))\n",
    "\n",
    "fig.add_trace(go.Histogram(x=results_df[results_df['true_label'] == True]['predicted_proba'],\n",
    "                           name='Actual Churn',\n",
    "                           opacity=0.7,\n",
    "                           nbinsx=30))\n",
    "\n",
    "fig.update_layout(title='Prediction Confidence Distribution',\n",
    "                  xaxis_title='Predicted Probability of Churn',\n",
    "                  yaxis_title='Count',\n",
    "                  barmode='overlay')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimization Opportunities\n",
    "\n",
    "### Areas to Improve Model Performance:\n",
    "\n",
    "#### 1. **Hyperparameter Tuning**\n",
    "- `iterations`: Try [200, 500, 1000] for better convergence\n",
    "- `learning_rate`: Experiment with [0.01, 0.03, 0.05, 0.1]\n",
    "- `depth`: Test [4, 6, 8, 10] to balance complexity\n",
    "- `l2_leaf_reg`: Add regularization [1, 3, 5, 7, 9]\n",
    "- `border_count`: Try [32, 64, 128, 254] for better splits\n",
    "\n",
    "#### 2. **Class Imbalance Handling**\n",
    "- Current churn rate: ~26.5%\n",
    "- Options:\n",
    "  - `class_weights`: Auto-balance classes\n",
    "  - `scale_pos_weight`: Boost minority class\n",
    "  - SMOTE oversampling\n",
    "  - Adjust decision threshold (optimize for F1 or recall)\n",
    "\n",
    "#### 3. **Feature Engineering**\n",
    "- Create interaction features (tenure × charges)\n",
    "- Polynomial features for numeric columns\n",
    "- Temporal features (if date data available)\n",
    "- Customer segmentation clusters\n",
    "\n",
    "#### 4. **Feature Selection**\n",
    "- Remove low-importance features (<1% importance)\n",
    "- Check for multicollinearity\n",
    "- Use recursive feature elimination\n",
    "\n",
    "#### 5. **Ensemble Methods**\n",
    "- Stack CatBoost with LightGBM/XGBoost\n",
    "- Voting classifier\n",
    "- Use cross-validation for robustness\n",
    "\n",
    "#### 6. **Business-Focused Optimization**\n",
    "- Optimize for **recall** if cost of missing churner is high\n",
    "- Optimize for **precision** if retention campaign cost is high\n",
    "- Use custom loss function based on business metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning Example\n",
    "\n",
    "Let's try a better configuration based on insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized model configuration\n",
    "optimized_model = CatBoostModel(\n",
    "    iterations=500,                # More iterations\n",
    "    learning_rate=0.03,            # Lower learning rate for stability\n",
    "    depth=8,                       # Deeper trees\n",
    "    l2_leaf_reg=3,                 # L2 regularization\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    # Additional params\n",
    "    eval_metric='AUC',\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "print(\"Training optimized model...\")\n",
    "optimized_model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "print(\"✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate optimized model\n",
    "y_pred_opt = optimized_model.predict(X_test)\n",
    "y_pred_proba_opt = optimized_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "optimized_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_opt),\n",
    "    'Precision': precision_score(y_test, y_pred_opt),\n",
    "    'Recall': recall_score(y_test, y_pred_opt),\n",
    "    'F1 Score': f1_score(y_test, y_pred_opt),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_opt),\n",
    "}\n",
    "\n",
    "# Compare models\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': list(metrics.keys()),\n",
    "    'Baseline': list(metrics.values()),\n",
    "    'Optimized': list(optimized_metrics.values()),\n",
    "})\n",
    "comparison['Improvement'] = comparison['Optimized'] - comparison['Baseline']\n",
    "comparison['Improvement %'] = (comparison['Improvement'] / comparison['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations for Production\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Implement proper hyperparameter search**:\n",
    "   - Use Optuna or Ray Tune for automated tuning\n",
    "   - Track experiments in MLflow\n",
    "   - Use cross-validation (5-fold or stratified)\n",
    "\n",
    "2. **Monitor model performance**:\n",
    "   - Set up data drift detection\n",
    "   - Track prediction distributions\n",
    "   - Monitor feature importance shifts\n",
    "\n",
    "3. **Business integration**:\n",
    "   - Define actionable threshold for retention campaigns\n",
    "   - Calculate expected ROI per customer segment\n",
    "   - A/B test model predictions vs baseline\n",
    "\n",
    "4. **Model improvements**:\n",
    "   - Add more temporal features if data available\n",
    "   - Experiment with neural networks for comparison\n",
    "   - Build calibrated probability estimates\n",
    "\n",
    "5. **Deployment considerations**:\n",
    "   - Export model in production format (ONNX, pickle)\n",
    "   - Set up prediction API\n",
    "   - Implement monitoring and alerting\n",
    "   - Version control models in MLflow\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
